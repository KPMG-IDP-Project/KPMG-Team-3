{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_number = pd.read_csv(r\"C:\\Users\\Olga\\Becode_Olga\\KPMG-Team-3\\data_csv\\C\\jc_119_0.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Vectorizer to Extract Features for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "_stopwords = list(stopwords.words('dutch') + list(punctuation) + [\"les\",\"'s\",\"''\",\"``\",\"du\",\"la\",\"par\",\"et\",\"à\", \"aux\",\"«\",\"le\", \"des\"])\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Corpus of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = jc_number['nl_text'].to_list()\n",
    "X = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=6, init='k-means++', max_iter=100, n_init=1, verbose=True, random_state=42)\n",
    "km.fit(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Numbers Stored in Array Labels and in Added Column 'class' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True)\n",
    "jc_number['class'] = km.labels_.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Out the Keywords for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the complete text of each cluster, we can use an NLTK function to find out the most frequent words within each cluster\n",
    "\n",
    "text = {}\n",
    "file_ind = {}\n",
    "ind = []\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    oneDocument = posts[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument\n",
    "\n",
    "keywords = {}\n",
    "counts = {}\n",
    "for cluster in range(6):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent = [word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster] = freq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the 10 Keywords that are Unique to Each Cluster and Add them to the Column \"key_words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys={}\n",
    "for cluster in range(6):   \n",
    "    other_clusters = list(set(range(6))-set([cluster]))\n",
    "    keys_other_clusters = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique = set(keywords[cluster])-keys_other_clusters\n",
    "    unique_keys[cluster] = nlargest(15, unique, key=counts[cluster].get)\n",
    "\n",
    "jc_number['key_words'] = jc_number.apply( lambda row : unique_keys[row['class']], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframe to CSV for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_number.to_csv(r\"C:\\Users\\Olga\\Becode_Olga\\KPMG-Team-3\\data_csv\\C\\clustered\\jc_119_0_cluster.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dfb4a36d778f7a51d0a9a0433f85de491d3f8700a8c6a91e127333bb8600fd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
